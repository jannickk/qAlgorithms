Colourblind:
#000000 #252525 #171723 #004949 #490092 #920000 #8f4e00 #22cf22

#ffffff #676767 #006ddb #009999 #b66dff #ff6db6 #db6d00 #ffdf4d

check if max distance in RT is actually coerced

Bifurcating Dendrogram:
Für einen Datensatz werden zwei unterschiedliche Komponentisierungen durchgeführt, eine davon mit qPattern
Für qPattern wird ein eindeutiges Entscheidungskriterium formuliert.
Das Ähnlichkeitskriterium wird als ein System ausgedrückt, in dem der gesamte Datensatz immer
    an der unähnlichsten Stelle getrennt wird Bzw. an der ähnlichsten zusammengeführt wird. Diese
    Trennung wird für alle Punkte durchgeführt, so dass die 1. Gruppe der ganze Datensatz ist und
    alle Gruppen so lange getrennt werden, bis sie nur noch ein Feature enthalten.
Für jedes Feature wird der Gruppenverlauf als ein Vektor des Formats {Gruppe 1, Untergruppe in Bezug auf Gruppe 1} ausgedrückt.
    Es enthält ebenfalls einen Index, der angibt, ab welchem Punkt die Gruppierung abgeschlossen war
Die Gruppenähnlichkeit zwischen qPattern und dem anderen Algorithmus soll bestimmt werden.
    Für jedes Feature sind nur die finalen Komponenten des anderen Algorithmus verfügbar.
Nun lässt sich auf einer per-Feature Basis angeben, wie viele Schritte notwendig sind, bis die
    Komponentisierung durch qPattern eine Gruppe trennt, die durch den anderen Algorithmus 
    erstellt wurde.
Je später die Diskrepanzen auftreten, desto weniger unterschiedlich sind sich die Ergebnisse.
    Diese Unterscheidung funktioniert auch für Gruppen, die nur durch den verglichenen Algorithmus
    noch weiter getrennt wurden.

Offenes Problem: 
Abschätzung des Zentroidfehlers für vorzentroidierte Daten. Mögliche Lösung über neuen Binning-Algo.
Gibt es lineare Zshg nach einem Denoising?
Abschätzung des Zentroidfehlers über maximale Gauss-Überlappung von zwei benachbarten Zentroiden.

https://stackoverflow.com/questions/61906480/how-to-display-ggplotly-plots-with-dynamically-created-tabs-and-for-loops#
https://stackoverflow.com/questions/53444333/dynamic-tabsets-with-multiple-plots-r-markdown?rq=3

210311_C3_S7_I_1_pos ist komisch
    => Eine fehlerhafte Regression hat den DQSF beeinträchtigt

suche nach nicht kovertierten dateien wenn ein leeres Verzeichnis gelesen wurde

DQSB MID nur innerhalb von maxdist scans, MOD nich gauss, sondern -50% skalieren

https://massive.ucsd.edu/ProteoSAFe/datasets.jsp#%7B%22full_search_input%22%3A%22TOF%22%2C%22table_sort_history%22%3A%22createdMillis_dsc%22%2C%22query%22%3A%7B%7D%7D

Qualitätsparameter Komponentisierung: Innerhalb einer Komponente, erweitere erst so, dass alle 
Massen in jeder Komponente der Gruppe vertreten sind. Gib dann jeder Masse je Komponente die %
Übereinstimmung mit anderen Massen dieser Kategorie. Bsp:
M1 existiert in allen Komponenten, daher hat sie den Score 1.
M2 existiert nur in einer von zehn Komponenten. In der einen hat sie einen score von 0.1, den
neun 0.9.

Zusätzliches Kriterium fürs Binning: Wahrscheinlichkeit, dass in einem Bin zwei 
Verteilungen vorliegen (bestimme über Umstellen von vcrit)
    Nicht linear lösbares Problem

Kann der Anteil, den der Peak am EIC hat, auch für Korellation zwischen 
Datensätzen verwendet werden?

Test: Können die Intensitäten im Bin unterschieden werden?

Wo schlägt vcrit an, wenn ich den Datensatz nach Intensität sortiere?

Erstelle Felder für allgemeine Parameter der Probe wie Leitfähigkeit

C1_S1_W_MI_1/2/3 sind Standards zum reinen Flusswasser hinzugespiked.
Blanks1/2/3 sind Systemblanks: Reinstwasser
Die anderen Blanks sind mit Standards gespiked.

Nur positiv-modus

Standards (u.a.):
Name		Masse		RT(min)	RT(s)	Addukt
Diclofenac	324.0260	14.4	864	M + H+
Sulfamethoxazol	258.0845	7.53	451.8	M + H+
Metoprolol	275.2352	7.50	450	M + 2H+
Carbamazepin	245.1525	10.13	607.8	M + H+

Je nach Datensatz kann es sinnvoll sein, auch "halbe" Peaks, bei denen 
das Intensitätsprofil nach dem höchsten Wert abbricht, mit einzubeziehen

https://www.agner.org/optimize/optimizing_cpp.pdf

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0125108

https://link.springer.com/article/10.1007/s41060-021-00275-z#Sec2

großen Multistandard-Datensatz anfragen (bfg - Datenbank)

Bin Reassembly sollte erst nach Aufreinigung der zu großen Bins passieren, diese müssen aber 
    von der Rekonstruktion ausgeschlossen werden

def multi_fit(Y):

    """

    Fits the piecewise quadratic model to multiple signal traces.

    Parameters:

        Y (array): Log-transformed intensity data.

    Returns:

        beta (array): Regression coefficients.

        Y_hat (array): Predicted values.

        rss (float): Residual sum of squares.

        n (integer): Number of data points.

    """  

    k, n = Y.shape // k = number of cols, n = number of rows

    # check if y is odd

    if n % 2 == 0:

        #drop the last element

        Y = Y[:, :-1]

        n -= 1

 
    scale = (n - 1)//2

    X_base, _ = piecewise_design_matrix(scale)


    # Build combined design matrix

    X_combined = np.zeros((k * n, k + 3)) // k*n rows and k + 3 columns

    for i in range(k):

        X_combined[i*n:(i+1)*n, i] = 1

        X_combined[i*n:(i+1)*n, k:] = X_base[:, 1:]

 
    # chop if y contains zeros

    y_is_flattened = False

    # if 0 in Y:

    Ycopy = Y.copy()

    Y = Y.flatten() // converts Y to a flat array by adding all rows together in order

    X_combined = X_combined[Y != 0, :]  

    Y = Y[Y != 0]

    y_is_flattened = True

 // at this point, both X_combined and Y only contain non-zero values.

    beta, _, _, _ = np.linalg.lstsq(X_combined, Y, rcond=None)

    # Re-Fit first k columns of beta while fixing the other coefficients and use exp(Y) as data

    shared_part = X_combined[:, k:] @ beta[k:]
	
	// this does matrix multiplication with the vector 

    A = np.exp(shared_part)  # shared part of the model

    beta_new = beta.copy()

    for i in range(k):

        idx = X_combined[:, i] == 1

        A_i = A[idx]

        Y_i_exp = np.exp(Y[idx]) - 1

        c_i = np.sum(A_i * Y_i_exp) / np.sum(A_i**2)

        beta_new[i] = np.log(c_i)

    beta = beta_new

    Y_hat = (X_combined @ beta)

    rss = np.sum((np.exp(Y) - np.exp(Y_hat)) ** 2)

    return beta, Y_hat, rss, len(Y)