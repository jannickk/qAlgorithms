---
title: "Test for calculating the tanimoto-score between two chromatographic peaks"
author: "Daniel HÃ¶hn"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# library(pracma) # required for dawson's integral
```

Notes: for SP_DAA_1, peak 3455 shows exceptionally many good fits

```{r read features}
# bins = read_csv("~/Work/ODEA/qAlgorithms/build/22090901_H2O_1_pos_positive_bins.csv")
# peaks = read_csv("~/Work/ODEA/qAlgorithms/build/22090901_H2O_1_pos_positive_features.csv")
# bins = read_csv("C:/Users/unisys/Documents/Studium/qAlgorithms/build/20240805_AA_DK_02_Ibu_Kali_0_20240808125236_positive_bins.csv")
# peaks = read_csv("C:/Users/unisys/Documents/Studium/qAlgorithms/build/20240805_AA_DK_02_Ibu_Kali_0_20240808125236_positive_features.csv")
bins = read_csv("C:/Users/unisys/Documents/paper_afints/SP_DDA_P1_positive_bins.csv")
peaks = read_csv("C:/Users/unisys/Documents/paper_afints/SP_DDA_P1_positive_features.csv")
peaks = peaks[order(peaks$retentionTime),]

peaks$suspect = c(abs(diff(peaks$mz)) < 1.5, FALSE)
sus = which(peaks$suspect & peaks$height > 100) 

```

```{r move and scale}
# these functions return a vector of length 7, with c(switch, b0n1, b0n2, b1n1, b1n2, b2, b3) for the moved regression

moveScale = function(feature, apex, scaleTo){
  # native feature properties
  b0 = feature$b0
  b1 = feature$b1
  b2 = feature$b2
  b3 = feature$b3
  AL = feature$apexLeft
  apexDist = -b1 / (2 * ifelse(AL, b2, b3)) # the x-position of the apex relative to the switch position
  switchPos = 0
  
  # re-scale the feature so that the height of the exponential form is equal to "scaleTo"
  height = b0 + b1 * apexDist + ifelse(AL, b2, b3) * apexDist^2
  # scaleTo = scaleTo / exp(height)
  # b0 = b0 + log(scaleTo) # division by the scalar in exponential form
  # equivalent to:
  b0 = b0 + log(scaleTo) - height
  
  # move the feature so that the apex is at the desired x
  offset_x = apex - apexDist
  b0_n1 = b0 - b1 * offset_x + b2 * offset_x * offset_x 
  b1_n1 = b1 - 2 * b2 * offset_x
  b0_n2 = b0 - b1 * offset_x + b3 * offset_x * offset_x 
  b1_n2 = b1 - 2 * b3 * offset_x
  
  # the new switch is the apex minus the apex distance
  switchPos = ifelse(AL, -b1_n1 / (2 * b2), -b1_n2 / (2 * b3)) - apexDist
  
  return(c(switchPos, b0_n1, b0_n2, b1_n1, b1_n2, b2, b3))
}

displayFeature = function(x, coeffs){
  left_half = x < coeffs[1]
  b0_l = coeffs[2] * as.numeric(left_half)
  b0_r = coeffs[3] * as.numeric(!left_half)
  b1_l = coeffs[4] * as.numeric(left_half) * x
  b1_r = coeffs[5] * as.numeric(!left_half) * x
  b2 = coeffs[6] * as.numeric(left_half) * x^2
  b3 = coeffs[7] * as.numeric(!left_half) * x^2

  return(exp(b0_l + b0_r + b1_l + b1_r + b2 + b3))
}
```

```{r select points}
# idx = sus[18]
# 
# feature_A = peaks$ID[idx]
# feature_B = peaks$ID[idx + 1]
# if(feature_A > feature_B) {idx = feature_A; feature_A = feature_B; feature_B = idx}
# 
# feature_A = peaks[which(peaks$ID == feature_A),]
# feature_B = peaks[which(peaks$ID == feature_B),]
# # feature_A$rt_switch = rt_switch(feature_A)
# # feature_B$rt_switch = rt_switch(feature_B)
# 
# points_A = bins[which(bins$binID == feature_A$binID),]
# points_B = bins[which(bins$binID == feature_B$binID),]
# 
# clamp_l = 0
# clamp_r = length(points_A$binID)
# if (feature_A$binIdxStart - 3 > 0)
# {clamp_l = feature_A$binIdxStart - 3}
# if (feature_A$binIdxEnd + 3 < clamp_r)
# {clamp_r = feature_A$binIdxEnd + 3}#
# points_A = points_A[clamp_l:clamp_r,]
# 
# clamp_l = 0
# clamp_r = length(points_B$binID)
# if (feature_B$binIdxStart - 3 > 0)
# {clamp_l = feature_B$binIdxStart - 3}
# if (feature_B$binIdxEnd + 3 < clamp_r)
# {clamp_r = feature_B$binIdxEnd + 3}
# points_B = points_B[clamp_l:clamp_r,]

clamp = function(points, binIdxStart, binIdxEnd){
  clamp_l = 0
  clamp_r = length(points$binID)
  if (binIdxStart - 3 > 0)
  {clamp_l = binIdxStart - 3}
  if (binIdxEnd + 3 < clamp_r)
  {clamp_r = binIdxEnd + 3}
  return(points[clamp_l:clamp_r,])
}

# points_A$ID = feature_A$ID
# points_B$ID = feature_B$ID
# points_A$area = points_A$area / feature_A$height
# points_B$area = points_B$area / feature_B$height
# merge = rbind(points_A, points_B)
# 
# ggplot(merge)+
#   geom_point(aes(retentionTime, area, colour = as.factor(ID)))+
#   # geom_point(aes(retentionTime, 0))+
#   stat_function(fun = displayFeature, args = list(moveScale(feature_A, feature_A$retentionTime, 1)), colour = "red3")+
#   stat_function(fun = displayFeature, args = list(moveScale(feature_B, feature_B$retentionTime, 1)), colour = "blue")+
#   geom_vline(xintercept = intersects)+
#   labs(caption = paste0("Tanimoto = ", tanimoto(feature_A, feature_B),
#     " ### m/z values: ", feature_A$ID, ": ", feature_A$mz, " | ", feature_B$ID, ": ", feature_B$mz))
```

```{r tanimoto-score:}

integral_exp = function(x, coeff){
  # calculate the integral of a function snippet
  # condition: both limits are greater or smaller than the switch point
  righthalf = x > coeff[1]
  b0 = coeff[2 + righthalf]
  b1 = coeff[4 + righthalf]
  b2 = coeff[6 + righthalf]
  
  term1 = sqrt(pi) * exp((b0 - b1^2) / (4 * b2)) 
  term2 = 0
  # if b2 is negative, the normal error function has to be used. If it is positive, the imaginary error function erfi is used instead
  if (b2 < 0){
    term2 = erf((2 * b2 * x + b1) / (2* sqrt(-b2))) * -1 / (2* sqrt(-b2))
  } else {
    term2 = erfi((2 * b2 * x + b1) / (2* sqrt(b2))) * 1 / (2* sqrt(b2))
  }
  # the exponential term gets extremely large, which is only compensated after substracting both 
  # error terms. Due to limits of numerical approximation, an interpolated approach is used
  return(term1 * term2)
}

full_height = function(x, coeff){
   righthalf = x > coeff[1]
   b0l = coeff[2] * (!righthalf)
   b0r = coeff[3] * (righthalf)
   b1l = coeff[4] * (!righthalf)
   b1r = coeff[5] * (righthalf)
   b2 = coeff[6] * (!righthalf)
   b3 = coeff[7] * (righthalf)
   
   return(exp(b0l + b0r + b1l * x + b1r * x + b2 * x^2 + b3 * x^2))
}

approx_int = function(coeff, values){
  righthalf = values > coeff[1]
  b0 = coeff[2 + righthalf]
  b1 = coeff[4 + righthalf]
  b2 = coeff[6 + righthalf]
  size = length(values)
  step = values[-1] - values[-size]
  
  heights = exp(b0 + b1 * values + b2 * values^2)
  upper = (heights[-1] + heights[-size]) / 2
  areas = upper * step
  return(areas)
}

inters_root = function(b0A, b0B, b1A, b1B, b23A, b23B){
  return((b1A - b1B)^2 - 4* (b23A - b23B) * (b0A - b0B))
}

intersect = function(root, b1A, b1B, b23A, b23B){
  position = (b1B - b1A ) / (2 * (b23A - b23B)) + root / (2 * (b23A - b23B))
  if (is.nan(position)){
    return(Inf)
  }
  return(position)
}

# basic idea: scale both regressions to height 1, then compare the overlap in the areas. 
# The difference should follow a fixed distribution if the features have actually the same elution profile
# The score is 1 if both curves are identical

tanimoto = function(feature_A, feature_B){
  # scale regressions to 1 and move them to the real axis
  coeff_A = moveScale(feature_A, feature_A$retentionTime, 1)
  coeff_B = moveScale(feature_B, feature_B$retentionTime, 1) 
  A_left = coeff_A[1] < coeff_B[1]
  switchL = ifelse(A_left, coeff_A[1], coeff_B[1])
  switchR = ifelse(A_left, coeff_B[1], coeff_A[1])
  
  # find all points of the moved regressions where the area is calculated differently
  intersects = c(0) 
  num_intersects = 1
  
  ## left and left
  root_ll = inters_root(coeff_A[2], coeff_B[2], coeff_A[4], coeff_B[4], coeff_A[6], coeff_B[6])
  if (root_ll >= 0){
    # there is at least one intersect between the curves. However, it might be past the point where they are valid
    int_ll = intersect(root_ll, coeff_A[4], coeff_B[4], coeff_A[6], coeff_B[6])
    if (int_ll <= switchL & int_ll > -Inf) {
      intersects[num_intersects] = int_ll
      num_intersects = num_intersects + 1
    }
    int_ll = intersect(-root_ll, coeff_A[4], coeff_B[4], coeff_A[6], coeff_B[6])
    if (int_ll <= switchL & int_ll > -Inf) {
      intersects[num_intersects] = int_ll
      num_intersects = num_intersects + 1
    }
  }
  intersects[num_intersects] = switchL
  num_intersects = num_intersects + 1
  ## middle region
  root_lr = inters_root(coeff_A[2], coeff_B[3], coeff_A[4], coeff_B[5], coeff_A[6], coeff_B[7])
  if (root_lr >= 0){
    # there is at least one intersect between the curves. However, it might be past the point where they are valid
    int_lr = intersect(root_lr, coeff_A[4], coeff_B[5], coeff_A[6], coeff_B[7])
    if (int_lr >= switchL & int_lr <= switchR) {
      intersects[num_intersects] = int_lr
      num_intersects = num_intersects + 1
    }
    int_lr = intersect(-root_lr, coeff_A[4], coeff_B[5], coeff_A[6], coeff_B[7])
    if (int_lr >= switchL & int_lr <= switchR) {
      intersects[num_intersects] = int_lr
      num_intersects = num_intersects + 1
    }
  }
  intersects[num_intersects] = switchR
  num_intersects = num_intersects + 1
  ## right and right
  root_rr = inters_root(coeff_A[3], coeff_B[3], coeff_A[5], coeff_B[5], coeff_A[7], coeff_B[7])
  if (root_rr >= 0){
    # there is at least one intersect between the curves. However, it might be past the point where they are valid
    int_rr = intersect(root_rr, coeff_A[5], coeff_B[5], coeff_A[7], coeff_B[7])
    if (int_rr >= switchR & int_rr < Inf) {
      intersects[num_intersects] = int_rr
      num_intersects = num_intersects + 1
    }
    int_rr = intersect(-root_rr, coeff_A[5], coeff_B[5], coeff_A[7], coeff_B[7])
    if (int_rr >= switchR & int_rr < Inf) {
      intersects[num_intersects] = int_rr
      num_intersects = num_intersects + 1
    }
  }
  intersects[num_intersects] = min(feature_A$lowestRetentionTime, feature_B$lowestRetentionTime)
  intersects[num_intersects + 1] = max(feature_A$highestRetentionTime, feature_B$highestRetentionTime)
  intersects = sort(intersects)
 
  # with the intersects found, the integrals need to be calculated between every pair in the vector
  repeats = length(intersects)
  
  steps = NULL
  for (i in 1:(repeats-1)) {
    steps = c(steps, seq(intersects[i], intersects[i + 1], by = (intersects[i + 1] - intersects[i]) / 10))
  }
  steps = unique(steps)
  int_approx_A = approx_int(coeff_A, steps)
  int_approx_B = approx_int(coeff_B, steps)
   
  # scale by intensity of the maximum function term
  weightpoints = (steps[-1] + steps[-length(steps)]) / 2
  weight = pmax(full_height(weightpoints, coeff_A), full_height(weightpoints, coeff_B))
  w_iaA = int_approx_A * weight
  w_iaB = int_approx_B * weight
  w_uAB = pmin(w_iaA, w_iaB)
  w_iaA = sum(w_iaA)
  w_iaB = sum(w_iaB)
  w_uAB = sum(w_uAB)
  
  union_AB = pmin(int_approx_A, int_approx_B)
  int_approx_A = sum(int_approx_A)
  int_approx_B = sum(int_approx_B)
  union_AB = sum(union_AB)
  if (union_AB < 0) {stop()}
  AuB = int_approx_A + int_approx_B - union_AB
  AnB = union_AB
  
  score = AnB / AuB
  score2 = w_uAB / (w_iaA + w_iaB - w_uAB)
  
  if (score < 0 | score > 1) {
    print(score)
    stop()
  }
  return(score2)
}


showFeats = function(peaks, bins, i){
  go = T
  j = i + 1
  while (go) {
    feature_A = peaks[i,]
    feature_B = peaks[j,]
    points_A = bins[which(bins$binID == feature_A$binID),]
    points_B = bins[which(bins$binID == feature_B$binID),]
    points_A = points_A[feature_A$binIdxStart:feature_A$binIdxEnd,]
    points_B = points_B[feature_B$binIdxStart:feature_B$binIdxEnd,]
    
    points_A$ID = paste("A:", feature_A$ID)
    points_B$ID = paste("B:", feature_B$ID)
    points_A$area = points_A$area / feature_A$height
    points_B$area = points_B$area / feature_B$height
    merge = rbind(points_A, points_B)

    plot = ggplot(merge)+
      geom_point(aes(retentionTime, area, colour = as.factor(ID)))+
      stat_function(fun = displayFeature, args = list(moveScale(feature_A, feature_A$retentionTime, 1)), colour = "red3")+
      stat_function(fun = displayFeature, args = list(moveScale(feature_B, feature_B$retentionTime, 1)), colour = "blue")+
      labs(caption = paste0("Tanimoto = ", tanimoto(feature_A, feature_B),
           " ### m/z values: ", feature_A$ID, ": ", feature_A$mz, " | ", feature_B$ID, ": ", feature_B$mz))
    print(plot)
    input = readline()
    if (input == "#"){
      j = i + 2
      i = i + 1
    } else if (input == "+"){
      j = j + 1
    }
  }
}
```

Consider making the score scale, so that the overlap at the maximum is more relevant than 
distances between points at the peak borders. While the problem is alleviated somewhat by 
restricting the area of comparison to the function limits, sensitivity of the test will be
reduced when comparing two functions with different limits.

```{r modification of feature}
feature_base = peaks[300,]
feature_mod = feature_base

feature_mod$retentionTime = feature_mod$retentionTime + feature_mod$retentionTimeUncertainty

tanimoto(feature_base, feature_mod)

# adjust the position of the feature within the feature uncertainty
moveTR = 0
moveTL = 0
for (i in 1:length(peaks$ID)) {
  feature_base = peaks[i,]
  feature_mod = feature_base

  feature_mod$retentionTime = feature_base$retentionTime + feature_base$retentionTimeUncertainty
  moveTR[i] = tanimoto(feature_base, feature_mod)
  feature_mod$retentionTime = feature_base$retentionTime - feature_base$retentionTimeUncertainty
  moveTL[i] = tanimoto(feature_base, feature_mod)
}



# adjust b0 so that the apex height varies within the height uncertainty
# note: this doesn't work, since the peaks are scaled to be the same height again. Just varying the height does not change the peak!
# height is peak at position -b1^2 / 2b2

# adjust b1 or b2 instead, under the same constraint of the height uncertainty not being exceeded
shrinkT = 0
scaleT  = 0
for (i in 1:length(peaks$ID)) {
  feature_base = peaks[i,]
  feature_mod = feature_base
  
  b23 = ifelse(feature_base$apexLeft, feature_base$b2, feature_base$b3)
  # height = b0 - b1^2 / 4b2
  # b0 - height = b1^2 / 4b2
  # b2 = b1^2 / (4 * (b0 - height)) # additional problem: b2 and b3 need to be adjusted
  # b1 = sqrt((b0 - height) * 4b2) # always positive
  height = feature_base$height - feature_base$heightUncertainty
  feature_mod$b1 = sqrt((feature_base$b0 - height) * 4 * b23) * sign(feature_base$b1)
  feature_mod$height = height
  shrinkT[i] = tanimoto(feature_base, feature_mod)
  
  height = feature_base$height + feature_base$heightUncertainty
  feature_mod$b1 = sqrt((feature_base$b0 - height) * 4 * b23) * sign(feature_base$b1)
  feature_mod$height = height
  scaleT[i] = tanimoto(feature_base, feature_mod)
}


hist(moveTR, breaks = 40)
hist(moveTL, breaks = 40)
hist(shrinkT, breaks = 40)
hist(scaleT, breaks = 40)
```

Question: how does the tanimoto score change as a function of moving the peak? Uncertainty = sigma
```{r}
# simulation process:
feature_base = round(runif(1, 1, length(peaks$ID)))
feature_base = peaks[feature_base,]
movePoints = rnorm(1000, feature_base$retentionTime, feature_base$retentionTimeUncertainty)
feature_mod = feature_base
Tscores = 0
for (i in 1:length(movePoints)) {
  feature_mod$retentionTime = movePoints[i]
  Tscores[i] = tanimoto(feature_base, feature_mod)
}
```

