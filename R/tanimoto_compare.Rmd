---
title: "Test for calculating the tanimoto-score between two chromatographic peaks"
author: "Daniel HÃ¶hn"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# library(pracma) # required for dawson's integral
```

Notes: for SP_DAA_1, peak 3455 shows exceptionally many good fits

```{r read features}
# bins = read_csv("~/Work/ODEA/qAlgorithms/build/22090901_H2O_1_pos_positive_bins.csv")
# peaks = read_csv("~/Work/ODEA/qAlgorithms/build/22090901_H2O_1_pos_positive_features.csv")
# bins = read_csv("C:/Users/unisys/Documents/Studium/qAlgorithms/build/20240805_AA_DK_02_Ibu_Kali_0_20240808125236_positive_bins.csv")
# peaks = read_csv("C:/Users/unisys/Documents/Studium/qAlgorithms/build/20240805_AA_DK_02_Ibu_Kali_0_20240808125236_positive_features.csv")
bins = read_csv("C:/Users/unisys/Documents/paper_afints/SP_DDA_P1_positive_bins.csv")
peaks = read_csv("C:/Users/unisys/Documents/paper_afints/SP_DDA_P1_positive_features.csv")
peaks = peaks[order(peaks$retentionTime),]

peaks$suspect = c(abs(diff(peaks$mz)) < 1.5, FALSE)
sus = which(peaks$suspect & peaks$height > 100) 

```

```{r move and scale}
# these functions return a vector of length 7, with c(switch, b0n1, b0n2, b1n1, b1n2, b2, b3) for the moved regression

moveScale = function(feature, apex, scaleTo){
  # native feature properties
  b0 = feature$b0
  b1 = feature$b1
  b2 = feature$b2
  b3 = feature$b3
  AL = feature$apexLeft
  apexDist = -b1 / (2 * ifelse(AL, b2, b3)) # the x-position of the apex relative to the switch position
  switchPos = 0
  
  # re-scale the feature so that the height of the exponential form is equal to "scaleTo"
  height = b0 + b1 * apexDist + ifelse(AL, b2, b3) * apexDist^2
  scaleTo = scaleTo / exp(height)
  b0 = b0 + log(scaleTo) # division by the scalar in exponential form
  
  # move the feature so that the apex is at the desired x
  offset = apex - apexDist
  b0_n1 = b0 - b1 * offset + b2 * offset * offset 
  b1_n1 = b1 - 2 * b2 * offset
  b0_n2 = b0 - b1 * offset + b3 * offset * offset 
  b1_n2 = b1 - 2 * b3 * offset
  
  # the new switch is the apex minus the apex distance
  switchPos = ifelse(AL, -b1_n1 / (2 * b2), -b1_n2 / (2 * b3)) - apexDist
  
  return(c(switchPos, b0_n1, b0_n2, b1_n1, b1_n2, b2, b3))
}

displayFeature = function(x, coeffs){
  left_half = x < coeffs[1]
  b0_l = coeffs[2] * as.numeric(left_half)
  b0_r = coeffs[3] * as.numeric(!left_half)
  b1_l = coeffs[4] * as.numeric(left_half) * x
  b1_r = coeffs[5] * as.numeric(!left_half) * x
  b2 = coeffs[6] * as.numeric(left_half) * x^2
  b3 = coeffs[7] * as.numeric(!left_half) * x^2

  return(exp(b0_l + b0_r + b1_l + b1_r + b2 + b3))
}
```

```{r select points}
# idx = sus[18]
# 
# feature_A = peaks$ID[idx]
# feature_B = peaks$ID[idx + 1]
# if(feature_A > feature_B) {idx = feature_A; feature_A = feature_B; feature_B = idx}
# 
# feature_A = peaks[which(peaks$ID == feature_A),]
# feature_B = peaks[which(peaks$ID == feature_B),]
# # feature_A$rt_switch = rt_switch(feature_A)
# # feature_B$rt_switch = rt_switch(feature_B)
# 
# points_A = bins[which(bins$binID == feature_A$binID),]
# points_B = bins[which(bins$binID == feature_B$binID),]
# 
# clamp_l = 0
# clamp_r = length(points_A$binID)
# if (feature_A$binIdxStart - 3 > 0)
# {clamp_l = feature_A$binIdxStart - 3}
# if (feature_A$binIdxEnd + 3 < clamp_r)
# {clamp_r = feature_A$binIdxEnd + 3}#
# points_A = points_A[clamp_l:clamp_r,]
# 
# clamp_l = 0
# clamp_r = length(points_B$binID)
# if (feature_B$binIdxStart - 3 > 0)
# {clamp_l = feature_B$binIdxStart - 3}
# if (feature_B$binIdxEnd + 3 < clamp_r)
# {clamp_r = feature_B$binIdxEnd + 3}
# points_B = points_B[clamp_l:clamp_r,]

clamp = function(points, binIdxStart, binIdxEnd){
  clamp_l = 0
  clamp_r = length(points$binID)
  if (binIdxStart - 3 > 0)
  {clamp_l = binIdxStart - 3}
  if (binIdxEnd + 3 < clamp_r)
  {clamp_r = binIdxEnd + 3}
  return(points[clamp_l:clamp_r,])
}

# points_A$ID = feature_A$ID
# points_B$ID = feature_B$ID
# points_A$area = points_A$area / feature_A$height
# points_B$area = points_B$area / feature_B$height
# merge = rbind(points_A, points_B)
# 
# ggplot(merge)+
#   geom_point(aes(retentionTime, area, colour = as.factor(ID)))+
#   # geom_point(aes(retentionTime, 0))+
#   stat_function(fun = displayFeature, args = list(moveScale(feature_A, feature_A$retentionTime, 1)), colour = "red3")+
#   stat_function(fun = displayFeature, args = list(moveScale(feature_B, feature_B$retentionTime, 1)), colour = "blue")+
#   geom_vline(xintercept = intersects)+
#   labs(caption = paste0("Tanimoto = ", tanimoto(feature_A, feature_B),
#     " ### m/z values: ", feature_A$ID, ": ", feature_A$mz, " | ", feature_B$ID, ": ", feature_B$mz))
```

```{r tanimoto-score:}

integral_exp = function(x, coeff){
  # calculate the integral of a function snippet
  # condition: both limits are greater or smaller than the switch point
  righthalf = x > coeff[1]
  b0 = coeff[2 + righthalf]
  b1 = coeff[4 + righthalf]
  b2 = coeff[6 + righthalf]
  
  term1 = sqrt(pi) * exp((b0 - b1^2) / (4 * b2)) 
  term2 = 0
  # if b2 is negative, the normal error function has to be used. If it is positive, the imaginary error function erfi is used instead
  if (b2 < 0){
    term2 = erf((2 * b2 * x + b1) / (2* sqrt(-b2))) * -1 / (2* sqrt(-b2))
  } else {
    term2 = erfi((2 * b2 * x + b1) / (2* sqrt(b2))) * 1 / (2* sqrt(b2))
  }
  # the exponential term gets extremely large, which is only compensated after substracting both 
  # error terms. Due to limits of numerical approximation, an interpolated approach is used
  return(term1 * term2)
}

approx_int = function(coeff, values){
  righthalf = values > coeff[1]
  b0 = coeff[2 + righthalf]
  b1 = coeff[4 + righthalf]
  b2 = coeff[6 + righthalf]
  step = values[2] - values[1]
  
  heights = exp(b0 + b1 * values + b2 * values^2)
  upper = (heights[-1] + heights[-length(values)]) / 2
  areas = upper * step
  return(sum(areas))
}

inters_root = function(b0A, b0B, b1A, b1B, b23A, b23B){
  return((b1A - b1B)^2 - 4* (b23A - b23B) * (b0A - b0B))
}

intersect = function(root, b1A, b1B, b23A, b23B){
  (b1B - b1A ) / (2 * (b23A - b23B)) + root / (2 * (b23A - b23B))
}

# basic idea: scale both regressions to height 1, then compare the overlap in the areas. 
# The difference should follow a fixed distribution if the features have actually the same elution profile
# The score is 1 if both curves are identical

tanimoto = function(feature_A, feature_B){
  # scale regressions to 1 and move them to the real axis
  coeff_A = moveScale(feature_A, feature_A$retentionTime, 1)
  coeff_B = moveScale(feature_B, feature_B$retentionTime, 1) 
  A_left = coeff_A[1] < coeff_B[1]
  switchL = ifelse(A_left, coeff_A[1], coeff_B[1])
  switchR = ifelse(A_left, coeff_B[1], coeff_A[1])
  
  # find all points of the moved regressions where the area is calculated differently
  intersects = c(0) 
  num_intersects = 1
  
  ## left and left
  root_ll = inters_root(coeff_A[2], coeff_B[2], coeff_A[4], coeff_B[4], coeff_A[6], coeff_B[6])
  if (root_ll >= 0){
    # there is at least one intersect between the curves. However, it might be past the point where they are valid
    int_ll = intersect(root_ll, coeff_A[4], coeff_B[4], coeff_A[6], coeff_B[6])
    if (int_ll <= switchL) {
      intersects[num_intersects] = int_ll
      num_intersects = num_intersects + 1
    }
    int_ll = intersect(-root_ll, coeff_A[4], coeff_B[4], coeff_A[6], coeff_B[6])
    if (int_ll <= switchL) {
      intersects[num_intersects] = int_ll
      num_intersects = num_intersects + 1
    }
  }
  intersects[num_intersects] = switchL
  num_intersects = num_intersects + 1
  ## middle region
  root_lr = inters_root(coeff_A[2], coeff_B[3], coeff_A[4], coeff_B[5], coeff_A[6], coeff_B[7])
  if (root_lr >= 0){
    # there is at least one intersect between the curves. However, it might be past the point where they are valid
    int_lr = intersect(root_lr, coeff_A[4], coeff_B[5], coeff_A[6], coeff_B[7])
    if (int_lr >= switchL & int_lr <= switchR) {
      intersects[num_intersects] = int_lr
      num_intersects = num_intersects + 1
    }
    int_lr = intersect(-root_lr, coeff_A[4], coeff_B[5], coeff_A[6], coeff_B[7])
    if (int_lr >= switchL & int_lr <= switchR) {
      intersects[num_intersects] = int_lr
      num_intersects = num_intersects + 1
    }
  }
  intersects[num_intersects] = switchR
  num_intersects = num_intersects + 1
  ## right and right
  root_rr = inters_root(coeff_A[3], coeff_B[3], coeff_A[5], coeff_B[5], coeff_A[7], coeff_B[7])
  if (root_rr >= 0){
    # there is at least one intersect between the curves. However, it might be past the point where they are valid
    int_rr = intersect(root_rr, coeff_A[5], coeff_B[5], coeff_A[7], coeff_B[7])
    if (int_rr >= switchR) {
      intersects[num_intersects] = int_rr
      num_intersects = num_intersects + 1
    }
    int_rr = intersect(-root_rr, coeff_A[5], coeff_B[5], coeff_A[7], coeff_B[7])
    if (int_rr >= switchR) {
      intersects[num_intersects] = int_rr
      num_intersects = num_intersects + 1
    }
  }
  intersects[num_intersects] = min(feature_A$lowestRetentionTime, feature_B$lowestRetentionTime)
  intersects[num_intersects + 1] = max(feature_A$highestRetentionTime, feature_B$highestRetentionTime)
  intersects = sort(intersects)
 
  # with the intersects found, the integrals need to be calculated between every pair in the vector
  repeats = length(intersects)
  
  AuB = 0
  AnB = 0
  for (i in 1:(repeats-1)) {
    steps = seq(intersects[i], intersects[i + 1], by = (intersects[i + 1] - intersects[i]) / 10)
    int_approx_A = approx_int(coeff_A, steps)
    int_approx_B = approx_int(coeff_B, steps)
    union_AB = min(int_approx_B, int_approx_A)
    if (union_AB < 0) {stop()}
    AuB = AuB - union_AB + int_approx_A + int_approx_B
    AnB = AnB + union_AB
  }
  score = AnB / AuB
  if (score < 0 | score > 1) {stop()}
  return(score)
}


showFeats = function(peaks, bins, i){
  go = T
  j = i + 1
  while (go) {
    feature_A = peaks[i,]
    feature_B = peaks[j,]
    points_A = bins[which(bins$binID == feature_A$binID),]
    points_B = bins[which(bins$binID == feature_B$binID),]
    points_A = points_A[feature_A$binIdxStart:feature_A$binIdxEnd,]
    points_B = points_B[feature_B$binIdxStart:feature_B$binIdxEnd,]
    
    points_A$ID = paste("A:", feature_A$ID)
    points_B$ID = paste("B:", feature_B$ID)
    points_A$area = points_A$area / feature_A$height
    points_B$area = points_B$area / feature_B$height
    merge = rbind(points_A, points_B)

    plot = ggplot(merge)+
      geom_point(aes(retentionTime, area, colour = as.factor(ID)))+
      stat_function(fun = displayFeature, args = list(moveScale(feature_A, feature_A$retentionTime, 1)), colour = "red3")+
      stat_function(fun = displayFeature, args = list(moveScale(feature_B, feature_B$retentionTime, 1)), colour = "blue")+
      labs(caption = paste0("Tanimoto = ", tanimoto(feature_A, feature_B),
           " ### m/z values: ", feature_A$ID, ": ", feature_A$mz, " | ", feature_B$ID, ": ", feature_B$mz))
    print(plot)
    input = readline()
    if (input == "#"){
      j = i + 2
      i = i + 1
    } else if (input == "+"){
      j = j + 1
    }
  }
}
```

Consider making the score scale, so that the overlap at the maximum is more relevant than 
distances between points at the peak borders. While the problem is alleviated somewhat by 
restricting the area of comparison to the function limits, sensitivity of the test will be
reduced when comparing two functions with different limits.