---
title: "Untitled"
author: "Daniel HÃ¶hn"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r load data}
# dir = "C:/Users/unisys/Documents/Studium/qAlgorithms/build/"
dir = "C:/Users/unisys/Documents/Studium/Masterarbeit/"
polarity = "_positive_"
name = "SP_DDA_P7"
# name = "warburg_testdata"
# name = "210229_C1_S1_W_MI_2_pos"
# name = "20240805_AA_DK_02_Ibu_Kali_0_20240808125236"
file = str_c(dir, name, polarity)
file_feat = str_c(file, "features.csv")
file_bins = str_c(file, "bins.csv")
file_featCens = str_c(file, "featCen.csv")
file_compRegs = str_c(file, "components.csv")

features = read_csv(file_feat)
featCens = read_csv(file_featCens)
compRegs = read_csv(file_compRegs)
```

I am relatively certain that the previous efforts with the normalised intensity profiles only seemed promising due to
the differences between absolute intensities being minimal. Scaling itself seems to be the biggest problem in solving
shape similarity through the tanimoto index / score 

```{r}
compID = 1
mzDiffCount = 0
intensityDiffGreaterTwo = 0

for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
  for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
  }
  # 
  # print(
  #  ggplot(points)+
  #    geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
  #    labs(caption = paste0("compID: ", compID))
  #  )
  MZdiffs = c(outer(feat$mz, feat$mz, `-`))
  IntDiffs = c(outer(feat$area, feat$area, `-`))
  mzDiffCount[compID] = sum(abs(MZdiffs - 1.003355) < mean(feat$mzUncertainty))
  intensityDiffGreaterTwo[compID] = sum(IntDiffs > max(feat$area) / 2)
}
```


```{r}
sum(mzDiffCount != 0)
sum(intensityDiffGreaterTwo != 0)
sel = which(mzDiffCount != 0)
for (compID in sel) {
  feat = features[which(features$CompID == compID),]
   points = featCens[which(featCens$featureID %in% feat$ID),]
   for (i in 1:length(feat$ID)) {
     points$mz[which(points$featureID == feat$ID[i])] = feat$mz[i]
   }

   print(
   ggplot(points)+
     geom_point(aes(retentionTime, height, colour = as.factor(mz)))+
     labs(caption = paste0("compID: ", compID))
   )
}

# check number of true duplicate retention times
RTmatch = 0
for (compID in 1:max(features$CompID)) {
  feat = features[which(features$CompID == compID),]
  points = featCens[which(featCens$featureID %in% feat$ID),]
  RTmatch[compID] = (length(points$mz) - length(unique(points$retentionTime))) / length(points$mz)
}
```


```{r}
# the tanimoto index might not be a good idea to use here, how should scaling be handled?

test_a = points[which(points$featureID == 23376),]
test_b = points[which(points$featureID == 23378),][-1,]

feat_a = features[23376,]
feat_b = features[23378,]
# test_b = test_a + rnorm(17, 5000, 10)

test_a = test_a$area / feat_a$area
test_b = test_b$area / feat_b$area

asq_real = 0
bsq_real = 0
ab_real = 0
for (i in 1:17) {
  asq_real = asq_real + test_a[i]^2
  bsq_real = bsq_real + test_b[i]^2
  ab_real = ab_real + test_a[i] * test_b[i]
}
score_real_sqs = ab_real / (asq_real + bsq_real - ab_real)

exclude = 0
asum = 0
bsum = 0
for (i in 1:17) {
  asum = asum + test_a[i]
  bsum = bsum + test_b[i]
  exclude = exclude + min(test_a[i], test_b[i])
}

score_real_jc = exclude / (asum + bsum - exclude)

score_real_sqs
score_real_jc
################################################################################

diff_a = 0
diff_b = 0

for (i in 2:17) {
  diff_a[i] = test_a$area[i] / test_a$area[i-1]
  diff_b[i] = test_b$area[i] / test_b$area[i-1]
}
diff_a
diff_b

# randomised control for test viability
con_a_tmp = rnorm(17, 100, 5)
con_b_tmp = rnorm(17, 10, 6)
con_a = 0
con_b = 0
for (i in 2:17) {
  con_a[i] = con_a_tmp[i] / con_a_tmp[i-1]
  con_b[i] = con_b_tmp[i] / con_b_tmp[i-1]
}

asq_real = 0
bsq_real = 0
ab_real = 0
asq_fake = 0
bsq_fake = 0
ab_fake = 0
for (i in 1:16) {
  asq_real = asq_real + diff_a[i]^2
  bsq_real = bsq_real + diff_b[i]^2
  ab_real = ab_real + diff_a[i] * diff_b[i]
  asq_fake = asq_fake + con_a[i]^2
  bsq_fake = bsq_fake + con_b[i]^2
  ab_fake = ab_fake + con_a[i] * con_b[i]
}

score_real = ab_real / (asq_real + bsq_real - ab_real)
score_fake = ab_fake / (asq_fake + bsq_fake - ab_fake)

```

